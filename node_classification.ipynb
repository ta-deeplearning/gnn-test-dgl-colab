{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Training...\n",
      "Epoch 00000 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00001 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00002 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00003 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00004 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00005 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00006 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00007 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00008 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00009 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00010 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00011 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00012 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00013 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00014 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00015 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00016 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00017 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00018 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00019 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00020 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00021 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00022 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00023 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00024 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00025 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00026 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00027 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00028 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00029 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00030 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00031 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00032 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00033 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00034 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00035 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00036 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00037 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00038 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00039 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00040 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00041 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00042 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00043 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00044 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00045 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00046 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00047 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00048 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00049 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00050 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00051 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00052 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00053 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00054 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00055 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00056 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00057 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00058 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00059 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00060 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00061 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00062 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00063 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00064 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00065 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00066 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00067 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00068 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00069 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00070 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00071 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00072 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00073 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00074 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00075 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00076 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00077 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00078 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00079 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00080 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00081 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00082 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00083 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00084 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00085 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00086 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00087 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00088 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00089 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00090 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00091 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00092 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00093 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00094 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00095 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00096 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00097 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00098 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00099 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00100 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00101 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00102 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00103 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00104 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00105 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00106 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00107 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00108 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00109 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00110 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00111 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00112 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00113 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00114 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00115 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00116 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00117 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00118 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00119 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00120 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00121 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00122 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00123 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00124 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00125 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00126 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00127 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00128 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00129 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00130 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00131 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00132 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00133 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00134 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00135 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00136 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00137 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00138 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00139 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00140 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00141 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00142 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00143 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00144 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00145 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00146 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00147 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00148 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00149 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00150 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00151 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00152 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00153 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00154 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00155 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00156 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00157 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00158 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00159 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00160 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00161 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00162 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00163 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00164 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00165 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00166 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00167 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00168 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00169 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00170 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00171 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00172 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00173 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00174 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00175 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00176 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00177 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00178 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00179 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00180 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00181 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00182 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00183 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00184 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00185 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00186 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00187 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00188 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00189 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00190 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00191 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00192 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00193 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00194 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00195 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00196 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00197 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00198 | Loss inf | Accuracy 0.0960 \n",
      "Epoch 00199 | Loss inf | Accuracy 0.0960 \n",
      "Training with DGL built-in GraphConv module.\n",
      "Testing...\n",
      "Test accuracy 0.1130\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "from dgl.data import CoraGraphDataset, CiteseerGraphDataset, PubmedGraphDataset\n",
    "from dgl import AddSelfLoop\n",
    "import argparse\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_size, hid_size, out_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # TODO: Add two layers of graph convolutional network to `self.layers'\n",
    "        # HINT: use GraphConv module in dglnn\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(g, h)\n",
    "        return h\n",
    "    \n",
    "def evaluate(g, features, labels, mask, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n",
    "\n",
    "\n",
    "def train(g, features, labels, masks, model):\n",
    "    # define train/val samples, loss function and optimizer\n",
    "    train_mask = masks[0]\n",
    "    val_mask = masks[1]\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
    "    loss = torch.FloatTensor([float('inf')])\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        # TODO: \n",
    "        # 1) get logit from the model\n",
    "        # 2) calculate loss using the loss function\n",
    "        # 3) optimize the weights using the optimizer\n",
    "        acc = evaluate(g, features, labels, val_mask, model)\n",
    "        print(\"Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f} \"\n",
    "              . format(epoch, loss.item(), acc))\n",
    "\n",
    "    print(f'Training with DGL built-in GraphConv module.')\n",
    " \n",
    "# load and preprocess dataset\n",
    "transform = AddSelfLoop()  # by default, it will first remove self-loops to prevent duplication\n",
    "data = CoraGraphDataset(transform=transform)\n",
    "g = data[0]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "g = g.int().to(device)\n",
    "features = g.ndata['feat']\n",
    "labels = g.ndata['label']\n",
    "masks = g.ndata['train_mask'], g.ndata['val_mask'], g.ndata['test_mask']\n",
    "    \n",
    "# normalization\n",
    "degs = g.in_degrees().float()\n",
    "norm = torch.pow(degs, -0.5).to(device)\n",
    "norm[torch.isinf(norm)] = 0\n",
    "g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "# create GCN model    \n",
    "in_size = features.shape[1]\n",
    "out_size = data.num_classes\n",
    "model = GCN(in_size, 16, out_size).to(device)\n",
    "\n",
    "# model training\n",
    "print('Training...')\n",
    "train(g, features, labels, masks, model)\n",
    "\n",
    "# test the model\n",
    "print('Testing...')\n",
    "acc = evaluate(g, features, labels, masks[2], model)\n",
    "print(\"Test accuracy {:.4f}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ta-gnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48ca828c5fbf71d95b90e7ee1b3f2ae8a52b417fe4745726ea307495f1bbf2ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
